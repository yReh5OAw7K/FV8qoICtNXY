# Table of Abbreviations

| Abbreviation | Full Description |
| :---: | --- |
| A3C | Asynchronous Advantage Actor–Critic |
| AC | Actor–Critic |
| AI | Artificial Intelligence |
| API | Application Programming Interface |
| APV-MCTS | Asynchronous Policy and Value MCTS |
| ARS | Augmented Random Search |
| BC | Behavior Cloning |
| CDF | Cumulative Distribution Function |
| CG | Conjugate Gradient |
| CNN | Convolutional Neural Network |
| CPU | Central Processing Unit |
| CTMDP | Continuous-Time Markov Decision Process |
| CTMP | Continuous-Time Markov Process |
| CTMRP | Continuous-Time Markov Reward Process |
| CTPOMDP | Continuous-Time Partially Observable Markov Decision Process |
| CTPOMP | Continuous-Time Partially Observable Markov Process |
| CTPOMRP | Continuous-Time Partially Observable Markov Reward Process |
| CTSMDP | Continuous-Time Semi-Markov Decision Process |
| CTSMP | Continuous-Time Semi-Markov Process |
| CTSMRP | Continuous-Time Semi-Markov Reward Process |
| DDPG | Deep Deterministic Policy Gradient |
| DL | Deep Learning |
| DP | Dynamic Programming |
| DPG | Deterministic Policy Gradient |
| DQN | Deep Q Network |
| DRL | Deep Reinforcement Learning |
| DTMDP | Discrete-Time Markov Decision Process |
| DTMP | Discrete-Time Markov Process |
| DTMRP | Discrete-Time Markov Reward Process |
| DTPOMDP | Discrete-Time Partially Observable Markov Decision Process |
| DTPOMP | Discrete-Time Partially Observable Markov Process |
| DTPOMRP | Discrete-Time Partially Observable Markov Reward Process |
| DTSMDP | Discrete-Time Semi-Markov Decision Process |
| DTSMP | Discrete-Time Semi-Markov Process |
| DTSMRP | Discrete-Time Semi-Markov Reward Process |
| ES | Evolution Strategy |
| FIM | Fisher Information Matrix |
| GAIL | Generative Adversarial Imitation Learning |
| GAE | Generalized Advantage Estimate |
| GAN | Generative Adversarial Network |
| GP | Gaussian Process |
| GPT | Generative Pre-trained Transformer |
| GPU | Graphics Processing Unit |
| HRL | Hierarchical Reinforcement Learning |
| IL | Imitation Learning |
| IQN | Implicit Quantile Networks |
| IRL | Inverse Reinforcement Learning |
| JSD | Jensen-Shannon Divergence |
| KLD | Kullback–Leibler Divergence |
| MAB | Multi-Arm Bandit |
| MARL | Multi-Agent Reinforcement Learning |
| MC | Monte Carlo |
| MCTS | Monte Carlo Tree Search |
| MDP | Markov Decision Process |
| ML | Machine Learning |
| MP | Markov Process |
| MRP | Markov Reward Process |
| MSE | Mean Squared Error |
| MTRL | Multi-Task Reinforcement Learning |
| NPG | Natural Policy Gradient |
| OffPAC | Off-Policy Actor–Critic |
| OPDAC | Off-Policy Deterministic Actor–Critic |
| OU | Ornstein Uhlenbeck |
| PbRL | Preference-based Reinforcement Learning |
| PBVI | Point-Based Value Iteration |
| PDF | Probability Distribution Function |
| PER | Prioritized Experience Replay |
| PG | Policy Gradient |
| PMF | Probability Mass Function |
| POMDP | Partially Observable Markov Decision Process |
| POMP | Partially Observable Markov Process |
| POMRP | Partially Observable Markov Reward Process |
| PPO | Proximal Policy Optimization |
| PUCT | Predictor-Upper Confidence bounds applied to Trees |
| QF | Quantile Function |
| QR | Quantile Regression |
| QR-DQN | Quantile Regression Deep Q Network |
| RAM | Random Access Memory |
| ReLU | Rectified Linear Unit |
| RL | Reinforcement Learning |
| RLHF | Reinforcement Learning with Human Feedback |
| RM | Reward Model |
| SAC | Soft Actor–Critic |
| SARSA | State-Action-Reward-State-Action |
| SGD | Stochastic Gradient Descent  |
| SMDP | Semi-Markov Decision Process |
| SMP | Semi-Markov Process |
| SMRP | Semi-Markov Reward Process |
| SOTA | State-Of-The-Art |
| SQL | Soft Q Learning |
| TD | Temporal Difference |
| TD3 | Twin Delay Deep Deterministic Policy Gradient |
| TPU | Tensor Processing Unit |
| TRM | Trust Region Method |
| TRPO | Trust Region Policy Optimization |
| TV | Total Variation |
| UCB | Upper Confidence Bound |
| UCB1 | Upper Confidence Bound 1 |
| UCBVI | Upper Confidence Bound Value Iteration |
| UCT | Upper Confidence bounds applied to Trees |
| VI | Value Iteration |
| VNM | von Neumann Morgenstern |
| VPG | Vanilla Policy Gradient |